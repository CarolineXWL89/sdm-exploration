{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SDMWordModel.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeC_pa_OvxID"
      },
      "source": [
        "# Setting up the runtime with proper installations + virtual environment for python 3.7 (restart kernel after running this block)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIJfIOtP5xM6"
      },
      "source": [
        "!python -m pip install --user virtualenv\n",
        "!apt-get install python3.7\n",
        "!apt-get install python3.7-venv\n",
        "!python3.7 -m  venv env\n",
        "!source env/bin/activate; pip install sdmlib; \n",
        "!pip install -U spacy\n",
        "!spacy download en_vectors_web_lg\n",
        "%env PYTHONPATH=$PATH:/content/env/lib/python3.7/site-packages:/root/.local/bin\n",
        "!echo $PATH\n",
        "\n",
        "#restart kernel from here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkJaNNiiv5Xp"
      },
      "source": [
        "# Data Pre-Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46Tau686_JGA"
      },
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/env/lib/python3.7/site-packages\")\n",
        "import numpy as np\n",
        "import sdmlib\n",
        "from sdmlib import Memory\n",
        "import struct\n",
        "import csv\n",
        "import spacy\n",
        "from sklearn.decomposition import PCA\n",
        "n_components = 15"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbSrhYtulOF-"
      },
      "source": [
        "## Helper Functions "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1M3eCtBcaJT"
      },
      "source": [
        "#source: https://stackoverflow.com/questions/16444726/binary-representation-of-float-in-python-bits-not-hex\n",
        "def to_bitstring(num):\n",
        "    packed = struct.pack('!f', num)\n",
        "    binaries = [bin(i) for i in packed]\n",
        "    stripped_binaries = [s.replace('0b', '') for s in binaries]\n",
        "    padded = [s.rjust(8, '0') for s in stripped_binaries]\n",
        "    return ''.join(padded)\n",
        "\n",
        "#turns a vector of real values into a binary vector\n",
        "def vec_bin(vec):\n",
        "    binary = []\n",
        "    for v in vec:\n",
        "        b = to_bitstring(v)\n",
        "        binary.extend([int(s) for s in b])\n",
        "    return binary\n",
        "\n",
        "#writes dictionary to csv\n",
        "def write_csv(dictionary, file):\n",
        "  w = csv.writer(open(file, \"w\"))\n",
        "  for key, val in dictionary.items():\n",
        "    lst = [key]\n",
        "    lst.extend(val)\n",
        "    w.writerow(lst)\n",
        "\n",
        "#reads dictionary from csv\n",
        "def read_csv(file):\n",
        "  vec_dictionary = {}\n",
        "  r = csv.reader(open(file, \"r\"))\n",
        "  for rows in r:\n",
        "    vec_dictionary[rows[0].lower()] = np.asarray([int(s) for s in rows[1:]], dtype=np.uint8)\n",
        "  return vec_dictionary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wm_-9c2QlSYe"
      },
      "source": [
        "## Get word vectors from dataset and perform PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ON8B6HHEcxBD"
      },
      "source": [
        "#get movie corpus from convokit: https://convokit.cornell.edu/documentation/movie.html\r\n",
        "!pip install -U convokit\r\n",
        "from convokit import Corpus, download\r\n",
        "#corpus = Corpus(filename=download(\"movie-corpus\"))\r\n",
        "#trying another dataset\r\n",
        "corpus = Corpus(filename=download(\"reddit-corpus-small\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCuWyS7ybNZK"
      },
      "source": [
        "import re\r\n",
        "\r\n",
        "text = \"\"\r\n",
        "length = 0\r\n",
        "for utt in corpus.iter_utterances():\r\n",
        "  convo = utt.text\r\n",
        "  convo = convo.replace(\"\\\"\", \"\").replace(\".\", \"\").replace(\",\", \"\").replace(\"?\", \"\").replace(\"!\", \"\")\r\n",
        "  # convo = convo.replace(\"[\\\".;!,:']\", \"\")\r\n",
        "  # convo = re.sub(r'\\b[.!,?;:]\\b', ' \\g<0> ', convo)\r\n",
        "  # convo = re.sub(r'\\B[.!,?;:]\\b', ' \\g<0> ', convo)\r\n",
        "  # convo = re.sub(r'\\b[.!,?;:]\\B', ' \\g<0> ', convo)\r\n",
        "  convo = convo.lower().split()\r\n",
        "  length += len(convo)\r\n",
        "  text += \" \" + ' '.join(convo)\r\n",
        "\r\n",
        "  #this is for computation sake\r\n",
        "  if length > 150000:\r\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHgaAB8DEzzl"
      },
      "source": [
        "#load in spacy vector package of word vectors\n",
        "nlp = spacy.load(\"en_vectors_web_lg\")\n",
        "tokens = nlp(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Frwq2n57Azj2",
        "outputId": "be351ea6-46d3-48de-d566-364b5f82cf15"
      },
      "source": [
        "#put all vectors in ndarray\n",
        "X = tokens[0].vector\n",
        "seen = [tokens[0].text]\n",
        "for i in range(1, len(tokens)):\n",
        "  if (tokens[i].has_vector) and (tokens[i].vector_norm != 0) and (tokens[i].text not in seen):\n",
        "    X = np.vstack((X, tokens[i].vector))\n",
        "    seen.append(tokens[i].text)\n",
        "\n",
        "#perform PCA to reduce dimensions from 300 to 30\n",
        "#if this is too costly for the SDM, lower n_components (and check to make sure the vectors are still unique)\n",
        "print(\"Performing PCA...\")\n",
        "pca = PCA(n_components=n_components)\n",
        "X_hat = pca.fit_transform(X)\n",
        "print(\"PCA Complete! Dimensions of X are now \", (X_hat.shape))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Performing PCA...\n",
            "PCA Complete! Dimensions of X are now  (11360, 15)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9we5Q0XgJkP"
      },
      "source": [
        "## Write bitstring to csv to save our RAM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIyAfF2pgHZc"
      },
      "source": [
        "threshold = np.mean(X)\r\n",
        "X_bool = X > threshold\r\n",
        "X_bool = np.array(X_bool, dtype=np.uint8)\r\n",
        "\r\n",
        "word2vec_bool = {}\r\n",
        "for i in range(X_bool.shape[0]):\r\n",
        "    word2vec_bool[seen[i]] = X_bool[i]\r\n",
        "    \r\n",
        "write_csv(word2vec_bool, \"word2vec_bool.csv\")\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pNZAUsrEtJQ"
      },
      "source": [
        "#convert vectors to bitstrings (15*32 dimensions) and create dictionary of (word, binary vectors)\n",
        "word2vec = {}\n",
        "for i in range(X_hat.shape[0]):\n",
        "    word2vec[seen[i]] = vec_bin(X_hat[i])\n",
        "\n",
        "#write to csv (this isn't super needed anymore, but in case we want it it's here)\n",
        "write_csv(word2vec, \"word2vec.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CaLySN1Bvti"
      },
      "source": [
        "#save dataset\r\n",
        "text_file = open(\"dataset.txt\", \"w\")\r\n",
        "text_file.write(text)\r\n",
        "text_file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDtufe0jlak0"
      },
      "source": [
        "## Read in vectors from CSV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrJtFpqcxWFU"
      },
      "source": [
        "word2vec_bool = read_csv(\"word2vec_bool.csv\")\r\n",
        "vec2word_bool = {}\r\n",
        "for key, val in word2vec_bool.items():\r\n",
        "  bitstring = ''.join([str(v) for v in val])\r\n",
        "  if (bitstring in vec2word_bool.keys()):\r\n",
        "    print(\"These two words have the same vector: \", key, vec2word_bool[bitstring])\r\n",
        "  vec2word_bool[bitstring] = key"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2ja7NB5A5Km"
      },
      "source": [
        "word2vec = read_csv(\"word2vec.csv\")\n",
        "\n",
        "#create vec->word dictionary\n",
        "vec2word = {}\n",
        "for key, val in word2vec.items():\n",
        "  bitstring = ''.join([str(v) for v in val])\n",
        "  if (bitstring in vec2word.keys()):\n",
        "    print(\"These two words have the same vector: \", key, vec2word[bitstring])\n",
        "  vec2word[bitstring] = key\n",
        "\n",
        "#word2vec and vec2word are essentially the same thing (just one has the word as the key and the other the bitstring)\n",
        "#word2vec stores the vector as a list of ints\n",
        "#vec2word stores the vector (the key) as a bitstring"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUtLPNaUlqCN"
      },
      "source": [
        "#Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMXLCanpkTwE"
      },
      "source": [
        "## Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpGsrczXNFp2"
      },
      "source": [
        "import pickle \r\n",
        "def vec2bitstring(vec):\r\n",
        "    bitstring = vec\r\n",
        "    if not isinstance(vec, str):\r\n",
        "      vec_list = [str(bit) for bit in vec.tolist()]\r\n",
        "      bitstring = ''.join(vec_list)\r\n",
        "    return bitstring\r\n",
        "\r\n",
        "def hamming(vec1, word2vec):\r\n",
        "    min_dist = float('inf')\r\n",
        "    best_vec = None\r\n",
        "    for key, vec in word2vec.items():\r\n",
        "        hamming_dist = np.sum(np.bitwise_xor(vec1, vec))\r\n",
        "        if hamming_dist < min_dist:\r\n",
        "            best_vec = vec\r\n",
        "            min_dist = hamming_dist\r\n",
        "    # print(f\"hamming: {min_dist}\")\r\n",
        "    return best_vec\r\n",
        "\r\n",
        "def store_model(db):\r\n",
        "    dbfile = open('model', 'ab') \r\n",
        "    # source, destination \r\n",
        "    pickle.dump(db, dbfile)                      \r\n",
        "    dbfile.close() \r\n",
        "\r\n",
        "def load_model():\r\n",
        "    dbfile = open('model', 'rb')      \r\n",
        "    db = pickle.load(dbfile) \r\n",
        "    dbfile.close()\r\n",
        "    return db \r\n",
        "\r\n",
        "def generate_scramble():\r\n",
        "    static_seed = np.random.randint(1000000)\r\n",
        "    return lambda x: np.random.RandomState(seed=static_seed).permutation(x)\r\n",
        "\r\n",
        "def history(word, prev_H, scramble, a, b):\r\n",
        "    num_bits = address_length if word is None else word.shape[0]\r\n",
        "    # print(\"num bits: \", num_bits)\r\n",
        "    cutoff = int(a*num_bits)\r\n",
        "    # print(\"cutoff: \", cutoff)\r\n",
        "    sprev_H = scramble(prev_H)\r\n",
        "    # print(\"scrambled history: \", len(sprev_H))\r\n",
        "    H = np.hstack((word[:cutoff], sprev_H[cutoff:]))\r\n",
        "    return np.asarray(H, dtype=np.uint8)\r\n",
        "\r\n",
        "def history_seq(word_seq, prev_H, scramble, a, b):\r\n",
        "    h = []\r\n",
        "    for word in word_seq:\r\n",
        "        # print(type(word))\r\n",
        "        #if word is None:\r\n",
        "        # continue\r\n",
        "        prev_H = history(word, prev_H, scramble, a, b)\r\n",
        "        h.append(prev_H)\r\n",
        "    return h\r\n",
        "\r\n",
        "def write(word, next_word, prev_H, scramble, a, b, unknown):\r\n",
        "    H = history(word, prev_H, scramble, a, b)\r\n",
        "    # print(\"H\", H.shape)\r\n",
        "    # print(\"word \", np.asarray(word).shape)\r\n",
        "    if unknown:\r\n",
        "        next_word = H\r\n",
        "    mem.write(H, next_word)\r\n",
        "    bitstring=vec2bitstring(next_word)\r\n",
        "    # print(vec2word.get(bitstring))\r\n",
        "    read_bitstr = read(H, scramble, a, b, 1)\r\n",
        "    # print(read_bitstr)\r\n",
        "    return H\r\n",
        "\r\n",
        "#\"The dog barked at the cat\"\r\n",
        "\r\n",
        "#\"The\" read H = (The, 000000000, scramble, a, b)\r\n",
        "#lol not done\r\n",
        "#\"The dog\" --> H\r\n",
        "def read(prev_H, scramble, a, b, num_words):\r\n",
        "    word_seq=[]\r\n",
        "    for i in range(num_words):\r\n",
        "        word = mem.read(prev_H)\r\n",
        "        bitstring = vec2bitstring(word)\r\n",
        "        if bitstring not in vec2word:\r\n",
        "            word = hamming(word, word2vec)\r\n",
        "            # print(type(word))\r\n",
        "            temp_bitstr = vec2word.get(vec2bitstring(word))\r\n",
        "            word_seq.append(temp_bitstr if type(word) is np.ndarray or temp_bitstr else \"unknown_word\")            \r\n",
        "        else:\r\n",
        "            word_seq.append(vec2word.get(bitstring))\r\n",
        "        prev_H = history(word, prev_H, scramble, a, b)\r\n",
        "    return \" \".join(word_seq)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ahe8lePG38MN"
      },
      "source": [
        "import time\r\n",
        "#training:\r\n",
        "# Set weights for history function\r\n",
        "scramble = generate_scramble()\r\n",
        "a = 0.25\r\n",
        "b = 1 - a\r\n",
        "\r\n",
        "#set up memory space\r\n",
        "address_length = len(word2vec['the'])\r\n",
        "N = address_length\r\n",
        "M = 1000000\r\n",
        "U = address_length\r\n",
        "T = 11000\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMH3MNfN-HwA"
      },
      "source": [
        "\r\n",
        "mem = Memory(N=N, M=M, U=U, d=None, T=T)\r\n",
        "fileObject = open(\"dataset.txt\", \"r\")\r\n",
        "data = fileObject.read().split()\r\n",
        "\r\n",
        "start_H = np.zeros(address_length, dtype=np.uint8)\r\n",
        "prev_H = start_H\r\n",
        "unknown_words = []\r\n",
        "punctuation = ['.', '?', '!', ';']\r\n",
        "\r\n",
        "start = time.perf_counter()\r\n",
        "for i in range(T):\r\n",
        "    unknown = False\r\n",
        "    got_data = data[i+1] in word2vec\r\n",
        "\r\n",
        "    #I think we should set the word vector to be = H for unknown words? but we would have to make sure it's unique...\r\n",
        "    #for now, making a list of unknown words\r\n",
        "    if got_data:\r\n",
        "        next_word = word2vec.get(data[i+1])\r\n",
        "    else:\r\n",
        "        next_word = np.zeros(address_length, dtype=np.uint8)\r\n",
        "        unknown_words.append(data[i+1])\r\n",
        "        unknown = True\r\n",
        "\r\n",
        "    got_data = data[i] in word2vec\r\n",
        "    if got_data and data[i].strip() in punctuation:\r\n",
        "        # print(f\"data[i]: {data[i]}; got_data: {got_data}\")\r\n",
        "        prev_H = start_H\r\n",
        "        continue\r\n",
        "    # print(\"not punctuation\")\r\n",
        "    word = word2vec.get(data[i]) if got_data else np.zeros(address_length, dtype=np.uint8)\r\n",
        "    prev_H = write(word, next_word, prev_H, scramble, a, b, unknown)\r\n",
        "\r\n",
        "    if unknown:\r\n",
        "      word2vec[data[i+1]] = prev_H\r\n",
        "      bitstring = ''.join([str(b) for b in prev_H])\r\n",
        "      vec2word[bitstring] = data[i+1]\r\n",
        "    if (i%1000 == 0):\r\n",
        "        end = time.perf_counter()\r\n",
        "        print(f\"{i} iteration completed in {end - start:0.4f} seconds\")\r\n",
        "store_model(mem)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEvIuZaLneiE"
      },
      "source": [
        "#store_model(mem)\r\n",
        "mem = load_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-CJh4EobSo6"
      },
      "source": [
        "#generating sequences\r\n",
        "import random \r\n",
        "\r\n",
        "tester_input = [] \r\n",
        "text_file = open(\"dataset.txt\", \"r\").readlines()[0]\r\n",
        "words = text_file.lower().replace(\"\\\"\", \"\").replace(\".\", \"\").replace(\",\", \"\").replace(\"?\", \"\").replace(\"!\", \"\").split()\r\n",
        "for _ in range(20):\r\n",
        "  word = random.choice(words)\r\n",
        "  # print(word)\r\n",
        "  if word not in tester_input:\r\n",
        "    tester_input.append(word)\r\n",
        "\r\n",
        "print(tester_input)\r\n",
        "prev_H = np.zeros(address_length)\r\n",
        "tester_vectors = [word2vec.get(w.lower()) for w in tester_input]\r\n",
        "histories = history_seq(tester_vectors, prev_H, scramble, a, b)\r\n",
        "for i in range(len(tester_input)):\r\n",
        "    #turn word into vector\r\n",
        "    word = read(histories[i], scramble, a, b, len(tester_input))\r\n",
        "    # print(f\"histories: {histories}\")\r\n",
        "    print(f\"{tester_input[i]}: {word}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9V-CaozCkXO3"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QO2tT99WcTjJ"
      },
      "source": [
        "import re\r\n",
        "text = []\r\n",
        "length = 0\r\n",
        "for utt in corpus.iter_utterances():\r\n",
        "  convo = utt.text\r\n",
        "  convo = convo.lower().split()\r\n",
        "  length += len(convo)\r\n",
        "  text.extend(convo)\r\n",
        "\r\n",
        "text = ' '.join(text)\r\n",
        "#obtain sentences from the dataset\r\n",
        "pat = re.compile(r'([a-z][^\\.!?;]*[\\.!?])', re.M)\r\n",
        "sentences = pat.findall(text)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrZ408Enf5HM"
      },
      "source": [
        "import math\r\n",
        "prev_H = np.zeros(address_length)\r\n",
        "sample = random.sample(sentences, 20)\r\n",
        "for s in sample:\r\n",
        "  s = s.replace(\"\\\"\", \"\").replace(\".\", \"\").replace(\",\", \"\").replace(\"?\", \"\").replace(\"!\", \"\")\r\n",
        "\r\n",
        "  s = s.lower()\r\n",
        "  print(\"Original sentence: \", s)\r\n",
        "  s = s.split()\r\n",
        "  s_vec = [word2vec.get(w.lower()) for w in s]\r\n",
        "  if len(s_vec) < 3:\r\n",
        "    continue\r\n",
        "  histories = history_seq(s_vec, prev_H, scramble, a, b)\r\n",
        "\r\n",
        "  #predict the rest of the sentence given the first third\r\n",
        "  print(\"generate the rest of the sentence given the first third of the sentence:\")\r\n",
        "  third_len = int(math.floor(0.3*len(s_vec)))\r\n",
        "  third_test = s_vec[:third_len]\r\n",
        "  h = histories[third_len-1]\r\n",
        "  word = read(h, scramble, a, b, len(s)-third_len)\r\n",
        "  start = ' '.join(s[:third_len])\r\n",
        "  print(f\"{start}: {word}\") len(s_vec)-twothird_len)  \r\n",
        "  start = ' '.join(s[:twothird_len])\r\n",
        "  print(f\"{start}: {word}\")\r\n",
        "  print(\"\\n\")\r\n",
        "  print()\r\n",
        "\r\n",
        "  #predict the rest of the sentence given the first half\r\n",
        "  print(\"generate the rest of the sentence given the first half of the sentence:\")\r\n",
        "  half_len = int(math.floor(0.5*len(s_vec)))\r\n",
        "  half_test = s_vec[:half_len]\r\n",
        "  h = histories[half_len-1]\r\n",
        "  word = read(h, scramble, a, b, len(s_vec)-half_len)  \r\n",
        "  start = ' '.join(s[:half_len])\r\n",
        "  print(f\"{start}: {word}\")\r\n",
        "  print()\r\n",
        "\r\n",
        "  #predict the rest of the sentence given the first half\r\n",
        "  print(\"generate the rest of the sentence given the first two-thirds of the sentence:\")\r\n",
        "  twothird_len = 2*third_len\r\n",
        "  half_test = s_vec[:twothird_len]\r\n",
        "  h = histories[twothird_len-1]\r\n",
        "  word = read(h, scramble, a, b,"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PV5MYHerkM_I"
      },
      "source": [
        "## Corpus Analytics\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6ZbSnc2dzWU"
      },
      "source": [
        "#https://towardsdatascience.com/very-simple-python-script-for-extracting-most-common-words-from-a-story-1e3570d0b9d0\r\n",
        "import collections\r\n",
        "file = open('dataset.txt', encoding=\"utf8\")\r\n",
        "a= file.read().lower().split()[:11000]\r\n",
        "\r\n",
        "# Instantiate a dictionary, and for every word in the file, \r\n",
        "# Add to the dictionary if it doesn't exist. If it does, increase the count.\r\n",
        "wordcount = {}\r\n",
        "# To eliminate duplicates, remember to split by punctuation, and use case demiliters.\r\n",
        "for word in a:\r\n",
        "    word = word.replace(\".\",\"\")\r\n",
        "    word = word.replace(\",\",\"\")\r\n",
        "    word = word.replace(\":\",\"\")\r\n",
        "    word = word.replace(\"\\\"\",\"\")\r\n",
        "    word = word.replace(\"!\",\"\")\r\n",
        "    word = word.replace(\"â€œ\",\"\")\r\n",
        "    word = word.replace(\"â€˜\",\"\")\r\n",
        "    word = word.replace(\"*\",\"\")\r\n",
        "    if word not in wordcount:\r\n",
        "        wordcount[word] = 1\r\n",
        "    else:\r\n",
        "        wordcount[word] += 1\r\n",
        "# Print most common word\r\n",
        "n_print = int(input(\"How many most common words to print: \"))\r\n",
        "print(\"\\nOK. The {} most common words are as follows\\n\".format(n_print))\r\n",
        "word_counter = collections.Counter(wordcount)\r\n",
        "for word, count in word_counter.most_common(n_print):\r\n",
        "    print(word, \": \", count)\r\n",
        "# Close the file\r\n",
        "file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Q9sYNyqNXEu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}